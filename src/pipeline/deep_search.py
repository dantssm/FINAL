# src/pipeline/deep_search.py
"""
OPTIMIZED Deep Search Pipeline - FREE VERSION
- Parallel search execution (3-5x faster)
- Jina AI Reader for scraping (2-3x faster)
- In-memory caching (instant on cache hits)
- Smarter chunking (fewer chunks = faster)
- All FREE services
"""

import asyncio
from typing import List, Dict, Optional
import hashlib
import os
import shutil
from datetime import datetime, timedelta
from fastapi import WebSocket

from src.config import config
from src.search.google_search import GoogleSearcher
from src.llm.openrouter_client import OpenRouterClient
from src.rag.vector_store import VectorStore
from src.cache.memory_cache import MemoryCache, CachedGoogleSearcher, CachedJinaScraper

class SearchLogger:
    """Tracks search progress"""
    
    def __init__(self):
        self.steps = []
    
    def log(self, message: str):
        timestamp = datetime.now().strftime("%H:%M:%S")
        self.steps.append(f"[{timestamp}] {message}")
        print(message)
    
    def get_steps(self):
        return self.steps


class SessionManager:
    """Manages session-based vector stores"""
    
    def __init__(self, base_path: str = "./data/sessions"):
        self.base_path = base_path
        self.sessions = {}
        self.session_last_access = {}
        
        os.makedirs(base_path, exist_ok=True)
        self._cleanup_old_sessions()
    
    def get_or_create_session(self, session_id: str) -> VectorStore:
        if session_id not in self.sessions:
            session_path = os.path.join(self.base_path, session_id)
            os.makedirs(session_path, exist_ok=True)
            self.sessions[session_id] = VectorStore(persist_directory=session_path)
            print(f"üìÅ Created session: {session_id[:8]}...")
        
        self.session_last_access[session_id] = datetime.now()
        return self.sessions[session_id]
    
    def delete_session(self, session_id: str):
        if session_id in self.sessions:
            del self.sessions[session_id]
            
            session_path = os.path.join(self.base_path, session_id)
            if os.path.exists(session_path):
                shutil.rmtree(session_path)
                print(f"üóëÔ∏è Deleted session: {session_id[:8]}...")
        
        if session_id in self.session_last_access:
            del self.session_last_access[session_id]
    
    def _cleanup_old_sessions(self, max_age_hours: int = 24):
        cutoff_time = datetime.now() - timedelta(hours=max_age_hours)
        
        if os.path.exists(self.base_path):
            for session_dir in os.listdir(self.base_path):
                session_path = os.path.join(self.base_path, session_dir)
                
                if os.path.isdir(session_path):
                    mtime = datetime.fromtimestamp(os.path.getmtime(session_path))
                    if mtime < cutoff_time:
                        shutil.rmtree(session_path)
                        print(f"üßπ Cleaned old session: {session_dir[:8]}...")
    
    def get_active_sessions_count(self) -> int:
        return len(self.sessions)


class DeepSearchPipeline:
    """
    OPTIMIZED Deep Search Pipeline with FREE services
    """
    
    def __init__(self):
        config.validate()
        
        # Initialize FREE in-memory cache
        self.cache = MemoryCache(default_ttl_hours=24, max_size=1000)
        
        # Initialize cached searcher
        self.google_searcher = CachedGoogleSearcher(
            config.GOOGLE_SEARCH_API_KEY,
            config.GOOGLE_CSE_ID,
            self.cache
        )
        
        # Use Jina AI Reader (FREE) with caching
        self.web_scraper = CachedJinaScraper(self.cache)
        
        # Use FREE OpenRouter model
        self.llm_client = OpenRouterClient(
            config.OPENROUTER_API_KEY,
            model_name=config.OPENROUTER_MODEL
        )
        
        # Session manager for RAG stores
        self.session_manager = SessionManager()
        self.session_histories = {}
        
        print(f"‚úÖ OPTIMIZED Deep Search Pipeline initialized (FREE)")
        print(f"   Model: {config.OPENROUTER_MODEL}")
        print(f"   Scraper: Jina AI Reader (FREE)")
        print(f"   Cache: In-Memory (FREE)")
    
    async def _send_websocket_progress(self, websocket: Optional[WebSocket], message: dict):
        """Send WebSocket message without blocking"""
        if not websocket:
            return
        
        try:
            await websocket.send_json(message)
        except:
            pass
    
    def generate_session_id(self) -> str:
        """Generate unique session ID"""
        timestamp = str(datetime.now().timestamp())
        random_str = os.urandom(8).hex()
        return hashlib.md5(f"{timestamp}{random_str}".encode()).hexdigest()[:16]
    
    async def search(
        self,
        query: str,
        session_id: Optional[str] = None,
        depth: int = 2,
        max_results_per_search: int = 7,
        use_rag: bool = True,
        websocket: Optional[WebSocket] = None
    ) -> Dict:
        """
        OPTIMIZED deep search with parallel execution
        """
        await self._send_websocket_progress(websocket, {
            "type": "status",
            "message": "üöÄ Starting optimized search...",
            "stage": "starting"
        })
        
        if not session_id:
            session_id = self.generate_session_id()
        
        vector_store = self.session_manager.get_or_create_session(session_id)
        logger = SearchLogger()
        
        logger.log(f"\nüîç Starting OPTIMIZED deep search")
        logger.log(f"   Query: '{query}'")
        logger.log(f"   Depth: {depth}, Max results: {max_results_per_search}")
        
        all_search_results = []
        
        # OPTIMIZATION 1: Generate all search queries upfront
        await self._send_websocket_progress(websocket, {
            "type": "status",
            "message": "üß† Planning search strategy...",
            "stage": "planning"
        })
        
        logger.log("üß† Generating diverse search queries...")
        
        # Generate multiple queries in parallel
        search_queries = await self.llm_client.generate_search_queries(
            query,
            num_queries=min(depth * 2, 4)  # Limit to 4 queries max
        )
        search_queries.insert(0, query)  # Original query first
        
        logger.log(f"   Generated {len(search_queries)} search queries")
        
        # OPTIMIZATION 2: Execute ALL searches in parallel
        await self._send_websocket_progress(websocket, {
            "type": "status",
            "message": f"üîç Searching {len(search_queries)} sources in parallel...",
            "stage": "parallel_search"
        })
        
        logger.log(f"üîç Executing {len(search_queries)} searches in PARALLEL...")
        
        # Run all Google searches concurrently
        search_tasks = [
            self.google_searcher.search(q, num_results=max_results_per_search)
            for q in search_queries
        ]
        all_results = await asyncio.gather(*search_tasks, return_exceptions=True)
        
        # OPTIMIZATION 3: Deduplicate and flatten results
        seen_urls = set()
        unique_results = []
        
        for results in all_results:
            if isinstance(results, list):
                for r in results:
                    if r['link'] not in seen_urls:
                        seen_urls.add(r['link'])
                        unique_results.append(r)
        
        logger.log(f"   Found {len(unique_results)} unique results")
        
        # OPTIMIZATION 4: Scrape top URLs in parallel (limit to best results)
        top_urls = [r['link'] for r in unique_results[:12]]  # Limit to 12 best
        
        await self._send_websocket_progress(websocket, {
            "type": "status",
            "message": f"üìÑ Extracting content from {len(top_urls)} pages...",
            "stage": "scraping"
        })
        
        logger.log(f"üìÑ Scraping {len(top_urls)} URLs with Jina Reader...")
        
        scraped_content = await self.web_scraper.scrape_multiple(top_urls)
        
        logger.log(f"   Successfully scraped {len(scraped_content)} pages")
        
        # Match scraped content with search results
        for result in unique_results:
            for scraped in scraped_content:
                if scraped['url'] == result['link']:
                    result['content'] = scraped['content']
                    
                    # Add to vector store
                    if use_rag:
                        await vector_store.add_document(
                            content=scraped['content'],
                            url=scraped['url'],
                            title=result['title'],
                            query=query
                        )
                    break
            
            if 'content' in result:
                all_search_results.append(result)
        
        # OPTIMIZATION 5: Only send top N sources to LLM (reduce tokens)
        top_sources = all_search_results[:8]  # Limit to 8 best sources
        
        await self._send_websocket_progress(websocket, {
            "type": "status",
            "message": f"üß† Analyzing {len(top_sources)} sources...",
            "stage": "analysis"
        })
        
        logger.log(f"üß† Generating answer from {len(top_sources)} sources...")
        
        # Generate comprehensive answer
        answer = await self.llm_client.generate_response(
            prompt=query,
            search_results=top_sources
        )
        
        # Store in session history
        if session_id not in self.session_histories:
            self.session_histories[session_id] = []
        
        self.session_histories[session_id].append({
            "query": query,
            "answer": answer,
            "sources": len(all_search_results),
            "timestamp": datetime.now().isoformat()
        })
        
        # Prepare response
        result = {
            "query": query,
            "answer": answer,
            "session_id": session_id,
            "sources": [
                {
                    "title": r['title'],
                    "url": r['link'],
                    "snippet": r.get('snippet', '')[:200]
                }
                for r in top_sources
            ],
            "total_sources": len(all_search_results),
            "search_depth": depth,
            "session_knowledge_size": vector_store.get_stats()['total_documents'],
            "cache_stats": self.cache.get_stats()
        }
        
        await self._send_websocket_progress(websocket, {
            "type": "complete",
            "message": "‚úÖ Search complete!",
            "stage": "complete",
            "data": result
        })
        
        logger.log(f"\n‚úÖ OPTIMIZED search complete!")
        logger.log(f"   Total sources: {result['total_sources']}")
        logger.log(f"   Cache stats: {result['cache_stats']['active_entries']} active entries")
        
        result["steps_log"] = logger.get_steps()
        return result
    
    async def chat(
        self,
        message: str,
        session_id: str,
        use_search: bool = True,
        use_rag: bool = True,
        websocket: Optional[WebSocket] = None
    ) -> str:
        """Chat with session context"""
        context = self._build_session_context(session_id)
        
        if use_search:
            result = await self.search(
                message,
                session_id=session_id,
                depth=1,
                max_results_per_search=5,
                use_rag=use_rag,
                websocket=websocket
            )
            return result['answer']
        elif use_rag:
            vector_store = self.session_manager.get_or_create_session(session_id)
            
            if vector_store.get_stats()['total_documents'] > 0:
                await self._send_websocket_progress(websocket, {
                    "type": "status",
                    "message": "üß† Searching session knowledge...",
                    "stage": "rag_search"
                })
                
                rag_results = await vector_store.search(message, n_results=5)
                response = await self.llm_client.generate_response(
                    prompt=message,
                    context=context,
                    search_results=rag_results
                )
            else:
                response = await self.llm_client.generate_response(
                    prompt=message,
                    context=context
                )
            return response
        else:
            response = await self.llm_client.generate_response(
                prompt=message,
                context=context
            )
            return response
    
    def _build_session_context(self, session_id: str) -> str:
        """Build context from session's history"""
        if session_id not in self.session_histories:
            return ""
        
        history = self.session_histories[session_id]
        if not history:
            return ""
        
        context_parts = []
        for item in history[-3:]:  # Last 3 interactions
            context_parts.append(f"Q: {item['query']}")
            context_parts.append(f"A: {item['answer'][:500]}...")
            context_parts.append("")
        
        return "\n".join(context_parts)
    
    def get_session_stats(self, session_id: str) -> Dict:
        """Get statistics for a specific session"""
        vector_store = self.session_manager.get_or_create_session(session_id)
        stats = vector_store.get_stats()
        
        stats['session_id'] = session_id
        stats['search_history_length'] = len(self.session_histories.get(session_id, []))
        stats['active_sessions'] = self.session_manager.get_active_sessions_count()
        stats['llm_model'] = self.llm_client.get_model_info()
        stats['cache_stats'] = self.cache.get_stats()
        
        return stats
    
    def get_knowledge_stats(self) -> Dict:
        """Get knowledge base statistics"""
        return {
            "cache_stats": self.cache.get_stats(),
            "active_sessions": self.session_manager.get_active_sessions_count()
        }
    
    def end_session(self, session_id: str):
        """End a session and clean up its data"""
        self.session_manager.delete_session(session_id)
        
        if session_id in self.session_histories:
            del self.session_histories[session_id]
        
        print(f"üëã Session {session_id[:8]}... ended")
    
    def clear_history(self):
        """Clear all session histories"""
        self.session_histories = {}
        self.llm_client.clear_history()
        print("‚úÖ All session histories cleared")