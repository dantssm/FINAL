# src/pipeline/deep_search.py
"""
OPTIMIZED Deep Search Pipeline - FREE VERSION
- Parallel search execution (3-5x faster)
- Jina AI Reader for scraping (2-3x faster)
- In-memory caching (instant on cache hits)
- Smarter chunking (fewer chunks = faster)
- All FREE services
"""

import asyncio
from typing import List, Dict, Optional
import hashlib
import os
import shutil
from datetime import datetime, timedelta
from fastapi import WebSocket

from src.config import config
from src.search.google_search import GoogleSearcher
from src.llm.openrouter_client import OpenRouterClient
from src.rag.vector_store import VectorStore
from src.cache.memory_cache import MemoryCache, CachedGoogleSearcher, CachedJinaScraper

class SearchLogger:
    """Tracks search progress with detailed logging"""
    
    def __init__(self):
        self.steps = []
        self.start_time = datetime.now()
    
    def log(self, message: str, level: str = "INFO"):
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        elapsed = (datetime.now() - self.start_time).total_seconds()
        formatted = f"[{timestamp}] [{elapsed:.2f}s] {message}"
        self.steps.append(formatted)
        
        # Color coding for console
        if level == "SUCCESS":
            print(f"âœ… {formatted}")
        elif level == "WARNING":
            print(f"âš ï¸  {formatted}")
        elif level == "ERROR":
            print(f"âŒ {formatted}")
        elif level == "PROGRESS":
            print(f"ğŸ”„ {formatted}")
        else:
            print(f"ğŸ“ {formatted}")
    
    def get_steps(self):
        return self.steps


class SessionManager:
    """Manages session-based vector stores"""
    
    def __init__(self, base_path: str = "./data/sessions"):
        self.base_path = base_path
        self.sessions = {}
        self.session_last_access = {}
        
        os.makedirs(base_path, exist_ok=True)
        self._cleanup_old_sessions()
        print(f"ğŸ“ Session Manager initialized at: {base_path}")
    
    def get_or_create_session(self, session_id: str) -> VectorStore:
        if session_id not in self.sessions:
            session_path = os.path.join(self.base_path, session_id)
            os.makedirs(session_path, exist_ok=True)
            self.sessions[session_id] = VectorStore(persist_directory=session_path)
            print(f"ğŸ“ Created new session: {session_id[:12]}... at {session_path}")
        
        self.session_last_access[session_id] = datetime.now()
        return self.sessions[session_id]
    
    def delete_session(self, session_id: str):
        if session_id in self.sessions:
            del self.sessions[session_id]
            
            session_path = os.path.join(self.base_path, session_id)
            if os.path.exists(session_path):
                shutil.rmtree(session_path)
                print(f"ğŸ—‘ï¸ Deleted session: {session_id[:12]}...")
        
        if session_id in self.session_last_access:
            del self.session_last_access[session_id]
    
    def _cleanup_old_sessions(self, max_age_hours: int = 24):
        cutoff_time = datetime.now() - timedelta(hours=max_age_hours)
        cleaned_count = 0
        
        if os.path.exists(self.base_path):
            for session_dir in os.listdir(self.base_path):
                session_path = os.path.join(self.base_path, session_dir)
                
                if os.path.isdir(session_path):
                    mtime = datetime.fromtimestamp(os.path.getmtime(session_path))
                    if mtime < cutoff_time:
                        shutil.rmtree(session_path)
                        cleaned_count += 1
        
        if cleaned_count > 0:
            print(f"ğŸ§¹ Cleaned {cleaned_count} old session(s) (>{max_age_hours}h)")
    
    def get_active_sessions_count(self) -> int:
        return len(self.sessions)


class DeepSearchPipeline:
    """
    OPTIMIZED Deep Search Pipeline with FREE services
    """
    
    def __init__(self):
        print("\n" + "="*60)
        print("ğŸš€ INITIALIZING DEEP SEARCH PIPELINE")
        print("="*60)
        
        config.validate()
        
        # Initialize FREE in-memory cache
        print("\nğŸ“¦ Setting up cache system...")
        self.cache = MemoryCache(
            default_ttl_hours=config.CACHE_TTL_HOURS,
            max_size=config.MAX_CACHE_SIZE
        )
        print(f"   âœ… Memory cache ready: {config.MAX_CACHE_SIZE} entries, {config.CACHE_TTL_HOURS}h TTL")
        
        # Initialize cached searcher
        print("\nğŸ” Setting up Google Search...")
        self.google_searcher = CachedGoogleSearcher(
            config.GOOGLE_SEARCH_API_KEY,
            config.GOOGLE_CSE_ID,
            self.cache
        )
        print(f"   âœ… Google Search ready with caching")
        
        # Use Jina AI Reader (FREE) with caching
        print("\nğŸŒ Setting up web scraper...")
        self.web_scraper = CachedJinaScraper(self.cache)
        print(f"   âœ… Jina AI Reader ready (1000 requests/day FREE)")
        
        # Use FREE OpenRouter model
        print("\nğŸ¤– Setting up LLM client...")
        self.llm_client = OpenRouterClient(
            config.OPENROUTER_API_KEY,
            model_name=config.OPENROUTER_MODEL
        )
        model_info = self.llm_client.get_model_info()
        print(f"   âœ… LLM ready: {model_info['model']}")
        print(f"   ğŸ“‹ Fallback models: {', '.join(model_info['fallback_models'][:2])}...")
        
        # Session manager for RAG stores
        print("\nğŸ’¾ Setting up session management...")
        self.session_manager = SessionManager()
        self.session_histories = {}
        print(f"   âœ… Session manager ready")
        
        print("\n" + "="*60)
        print("âœ… DEEP SEARCH PIPELINE READY!")
        print("="*60)
        print(f"Configuration Summary:")
        print(f"  â€¢ LLM Model: {config.OPENROUTER_MODEL}")
        print(f"  â€¢ Embeddings: {config.EMBEDDING_PROVIDER}")
        print(f"  â€¢ Web Scraper: Jina AI Reader")
        print(f"  â€¢ Cache: In-Memory ({config.MAX_CACHE_SIZE} entries)")
        print(f"  â€¢ Chunk Size: {config.CHUNK_SIZE} chars")
        print(f"  â€¢ Concurrent Scrapes: {config.MAX_CONCURRENT_SCRAPES}")
        print("="*60 + "\n")
    
    async def _send_websocket_progress(self, websocket: Optional[WebSocket], message: dict):
        """Send WebSocket message without blocking"""
        if not websocket:
            return
        
        try:
            await websocket.send_json(message)
        except:
            pass
    
    def generate_session_id(self) -> str:
        """Generate unique session ID"""
        timestamp = str(datetime.now().timestamp())
        random_str = os.urandom(8).hex()
        session_id = hashlib.md5(f"{timestamp}{random_str}".encode()).hexdigest()[:16]
        print(f"ğŸ†” Generated new session ID: {session_id}")
        return session_id
    
    async def search(
        self,
        query: str,
        session_id: Optional[str] = None,
        depth: int = 2,
        max_results_per_search: int = 7,
        use_rag: bool = True,
        websocket: Optional[WebSocket] = None
    ) -> Dict:
        """
        OPTIMIZED deep search with parallel execution
        """
        logger = SearchLogger()
        
        await self._send_websocket_progress(websocket, {
            "type": "status",
            "message": "ğŸš€ Starting optimized search...",
            "stage": "starting"
        })
        
        if not session_id:
            session_id = self.generate_session_id()
        
        logger.log("="*60)
        logger.log("ğŸ” STARTING DEEP SEARCH", "SUCCESS")
        logger.log("="*60)
        logger.log(f"Query: '{query}'")
        logger.log(f"Session ID: {session_id[:12]}...")
        logger.log(f"Search Depth: {depth} levels")
        logger.log(f"Max Results per Search: {max_results_per_search}")
        logger.log(f"Using RAG: {use_rag}")
        logger.log(f"Using Model: {config.OPENROUTER_MODEL}")
        logger.log(f"Using Embeddings: {config.EMBEDDING_PROVIDER}")
        logger.log("="*60)
        
        vector_store = self.session_manager.get_or_create_session(session_id)
        logger.log(f"ğŸ“š Vector store ready: {vector_store.get_stats()['total_documents']} existing documents")
        
        all_search_results = []
        
        # OPTIMIZATION 1: Generate all search queries upfront
        await self._send_websocket_progress(websocket, {
            "type": "status",
            "message": "ğŸ§  Planning search strategy...",
            "stage": "planning"
        })
        
        logger.log("\nğŸ§  PHASE 1: Query Generation", "PROGRESS")
        logger.log(f"Generating diverse search queries (target: {min(depth * 2, 6)} queries)...")
        
        # Generate multiple queries in parallel
        search_queries = await self.llm_client.generate_search_queries(
            query,
            num_queries=min(depth * 2, 6)  # Increased limit from 4 to 6
        )
        search_queries.insert(0, query)  # Original query first
        
        logger.log(f"âœ… Generated {len(search_queries)} search queries:", "SUCCESS")
        for i, q in enumerate(search_queries, 1):
            logger.log(f"   {i}. {q}")
        
        # OPTIMIZATION 2: Execute ALL searches in parallel
        await self._send_websocket_progress(websocket, {
            "type": "status",
            "message": f"ğŸ” Searching {len(search_queries)} sources in parallel...",
            "stage": "parallel_search"
        })
        
        logger.log(f"\nğŸ” PHASE 2: Parallel Google Search", "PROGRESS")
        logger.log(f"Executing {len(search_queries)} searches concurrently...")
        logger.log(f"Requesting {max_results_per_search} results per search...")
        
        # Run all Google searches concurrently
        search_tasks = [
            self.google_searcher.search(q, num_results=max_results_per_search)
            for q in search_queries
        ]
        all_results = await asyncio.gather(*search_tasks, return_exceptions=True)
        
        # OPTIMIZATION 3: Deduplicate and flatten results
        logger.log(f"\nğŸ“Š PHASE 3: Results Processing", "PROGRESS")
        seen_urls = set()
        unique_results = []
        
        for i, results in enumerate(all_results, 1):
            if isinstance(results, list):
                logger.log(f"Search {i}: Found {len(results)} results")
                for r in results:
                    if r['link'] not in seen_urls:
                        seen_urls.add(r['link'])
                        unique_results.append(r)
            else:
                logger.log(f"Search {i}: Failed with error", "WARNING")
        
        logger.log(f"âœ… Total unique URLs found: {len(unique_results)}", "SUCCESS")
        
        # OPTIMIZATION 4: Scrape URLs in parallel (no artificial limits)
        top_urls = [r['link'] for r in unique_results[:max_results_per_search * 3]]  # Increased from 12
        
        await self._send_websocket_progress(websocket, {
            "type": "status",
            "message": f"ğŸ“„ Extracting content from {len(top_urls)} pages...",
            "stage": "scraping"
        })
        
        logger.log(f"\nğŸ“„ PHASE 4: Content Extraction", "PROGRESS")
        logger.log(f"Scraping {len(top_urls)} URLs with Jina Reader...")
        logger.log(f"Concurrent scrapes: {config.MAX_CONCURRENT_SCRAPES}")
        
        scraped_content = await self.web_scraper.scrape_multiple(top_urls)
        
        logger.log(f"âœ… Successfully scraped {len(scraped_content)}/{len(top_urls)} pages", "SUCCESS")
        
        # Match scraped content with search results
        logger.log(f"\nğŸ’¾ PHASE 5: Knowledge Base Update", "PROGRESS")
        added_count = 0
        
        for result in unique_results:
            for scraped in scraped_content:
                if scraped['url'] == result['link']:
                    result['content'] = scraped['content']
                    
                    # Add to vector store
                    if use_rag:
                        await vector_store.add_document(
                            content=scraped['content'],
                            url=scraped['url'],
                            title=result['title'],
                            query=query
                        )
                        added_count += 1
                    break
            
            if 'content' in result:
                all_search_results.append(result)
        
        if use_rag:
            logger.log(f"âœ… Added {added_count} documents to vector store", "SUCCESS")
            logger.log(f"ğŸ“š Total documents in store: {vector_store.get_stats()['total_documents']}")
        
        # OPTIMIZATION 5: Use RAG to retrieve most relevant chunks
        await self._send_websocket_progress(websocket, {
            "type": "status",
            "message": f"ğŸ§  Searching vector store for most relevant content...",
            "stage": "rag_retrieval"
        })
        
        logger.log(f"\nğŸ§  PHASE 6: RAG Retrieval", "PROGRESS")
        
        if use_rag and vector_store.get_stats()['total_documents'] > 0:
            logger.log(f"ğŸ” Performing semantic search in vector store...")
            logger.log(f"   Query: '{query}'")
            logger.log(f"   Requesting top 20 most relevant chunks...")
            
            # Search vector store for most relevant chunks
            rag_results = await vector_store.search(query, n_results=20)
            
            logger.log(f"âœ… Retrieved {len(rag_results)} relevant chunks from vector store", "SUCCESS")
            
            if rag_results:
                logger.log(f"   Similarity scores: {rag_results[0]['similarity_score']:.3f} (top) to {rag_results[-1]['similarity_score']:.3f} (lowest)")
            
            # Use RAG results if available, otherwise fall back to all sources
            sources_for_llm = rag_results if rag_results else all_search_results[:15]
        else:
            logger.log(f"âš ï¸  No vector store available, using all scraped sources", "WARNING")
            sources_for_llm = all_search_results[:15]
        
        await self._send_websocket_progress(websocket, {
            "type": "status",
            "message": f"ğŸ§  Analyzing {len(sources_for_llm)} most relevant chunks...",
            "stage": "analysis"
        })
        
        logger.log(f"\nğŸ§  PHASE 7: Answer Generation", "PROGRESS")
        logger.log(f"Sending {len(sources_for_llm)} most relevant sources to LLM...")
        logger.log(f"Total content size: ~{sum(len(s.get('content', '')) for s in sources_for_llm):,} chars")
        
        # Generate comprehensive answer using most relevant content
        answer = await self.llm_client.generate_response(
            prompt=query,
            search_results=sources_for_llm
        )
        
        logger.log(f"âœ… Generated answer: {len(answer)} characters", "SUCCESS")
        
        # Store in session history
        if session_id not in self.session_histories:
            self.session_histories[session_id] = []
        
        self.session_histories[session_id].append({
            "query": query,
            "answer": answer,
            "sources": len(all_search_results),
            "timestamp": datetime.now().isoformat()
        })
        
        logger.log(f"ğŸ’¾ Saved to session history ({len(self.session_histories[session_id])} total)")
        
        # Prepare response
        cache_stats = self.cache.get_stats()
        vector_stats = vector_store.get_stats()
        
        result = {
            "query": query,
            "answer": answer,
            "session_id": session_id,
            "sources": [
                {
                    "title": r.get('title', 'No title'),
                    "url": r.get('url', r.get('link', '#')),
                    "snippet": r.get('content', r.get('snippet', ''))[:300]
                }
                for r in sources_for_llm[:10]  # Show top 10 sources used
            ],
            "total_sources": len(all_search_results),
            "chunks_analyzed": len(sources_for_llm) if use_rag else 0,
            "search_depth": depth,
            "session_knowledge_size": vector_stats['total_documents'],
            "cache_stats": cache_stats
        }
        
        await self._send_websocket_progress(websocket, {
            "type": "complete",
            "message": "âœ… Search complete!",
            "stage": "complete",
            "data": result
        })
        
        logger.log("\n" + "="*60)
        logger.log("âœ… SEARCH COMPLETE!", "SUCCESS")
        logger.log("="*60)
        logger.log(f"ğŸ“Š Results Summary:")
        logger.log(f"   â€¢ Total sources found: {result['total_sources']}")
        logger.log(f"   â€¢ Chunks analyzed by LLM: {result['chunks_analyzed']}")
        logger.log(f"   â€¢ Answer length: {len(answer)} characters")
        logger.log(f"   â€¢ Cache entries: {cache_stats['active_entries']}/{cache_stats['total_entries']}")
        logger.log(f"   â€¢ Vector DB docs: {vector_stats['total_documents']}")
        logger.log(f"   â€¢ Embedding provider: {vector_stats['embedding_provider']}")
        if use_rag:
            logger.log(f"   â€¢ RAG enabled: YES âœ… (used semantic search)")
        else:
            logger.log(f"   â€¢ RAG enabled: NO âŒ (direct scraping only)")
        elapsed_total = (datetime.now() - logger.start_time).total_seconds()
        logger.log(f"   â€¢ Total time: {elapsed_total:.2f}s")
        logger.log("="*60)
        
        result["steps_log"] = logger.get_steps()
        return result
    
    async def chat(
        self,
        message: str,
        session_id: str,
        use_search: bool = True,
        use_rag: bool = True,
        websocket: Optional[WebSocket] = None
    ) -> str:
        """Chat with session context"""
        print(f"\nğŸ’¬ Starting chat in session {session_id[:12]}...")
        print(f"   Message: '{message[:60]}...'")
        print(f"   Use search: {use_search}")
        print(f"   Use RAG: {use_rag}")
        
        context = self._build_session_context(session_id)
        
        if use_search:
            print("ğŸ” Triggering search for chat message...")
            result = await self.search(
                message,
                session_id=session_id,
                depth=1,
                max_results_per_search=5,
                use_rag=use_rag,
                websocket=websocket
            )
            print(f"âœ… Chat search complete: {len(result['answer'])} chars")
            return result['answer']
        elif use_rag:
            vector_store = self.session_manager.get_or_create_session(session_id)
            
            if vector_store.get_stats()['total_documents'] > 0:
                print(f"ğŸ“š Searching session knowledge base...")
                await self._send_websocket_progress(websocket, {
                    "type": "status",
                    "message": "ğŸ§  Searching session knowledge...",
                    "stage": "rag_search"
                })
                
                rag_results = await vector_store.search(message, n_results=5)
                print(f"âœ… Found {len(rag_results)} relevant chunks")
                response = await self.llm_client.generate_response(
                    prompt=message,
                    context=context,
                    search_results=rag_results
                )
            else:
                print("âš ï¸  No documents in vector store, using context only")
                response = await self.llm_client.generate_response(
                    prompt=message,
                    context=context
                )
            return response
        else:
            print("ğŸ’­ Using context only (no search, no RAG)")
            response = await self.llm_client.generate_response(
                prompt=message,
                context=context
            )
            return response
    
    def _build_session_context(self, session_id: str) -> str:
        """Build context from session's history"""
        if session_id not in self.session_histories:
            return ""
        
        history = self.session_histories[session_id]
        if not history:
            return ""
        
        context_parts = []
        for item in history[-5:]:  # Increased from 3 to 5
            context_parts.append(f"Q: {item['query']}")
            context_parts.append(f"A: {item['answer'][:800]}...")  # Increased from 500
            context_parts.append("")
        
        return "\n".join(context_parts)
    
    def get_session_stats(self, session_id: str) -> Dict:
        """Get statistics for a specific session"""
        vector_store = self.session_manager.get_or_create_session(session_id)
        stats = vector_store.get_stats()
        
        stats['session_id'] = session_id
        stats['search_history_length'] = len(self.session_histories.get(session_id, []))
        stats['active_sessions'] = self.session_manager.get_active_sessions_count()
        stats['llm_model'] = self.llm_client.get_model_info()
        stats['cache_stats'] = self.cache.get_stats()
        
        print(f"\nğŸ“Š Session Stats for {session_id[:12]}:")
        print(f"   â€¢ Documents: {stats['total_documents']}")
        print(f"   â€¢ Search history: {stats['search_history_length']} queries")
        print(f"   â€¢ Embedding provider: {stats['embedding_provider']}")
        
        return stats
    
    def get_knowledge_stats(self) -> Dict:
        """Get knowledge base statistics"""
        cache_stats = self.cache.get_stats()
        active_sessions = self.session_manager.get_active_sessions_count()
        
        print(f"\nğŸ“Š Knowledge Base Stats:")
        print(f"   â€¢ Active sessions: {active_sessions}")
        print(f"   â€¢ Cache entries: {cache_stats['active_entries']}/{cache_stats['total_entries']}")
        print(f"   â€¢ Cache size limit: {cache_stats['max_size']}")
        
        return {
            "cache_stats": cache_stats,
            "active_sessions": active_sessions
        }
    
    def end_session(self, session_id: str):
        """End a session and clean up its data"""
        print(f"\nğŸ‘‹ Ending session {session_id[:12]}...")
        self.session_manager.delete_session(session_id)
        
        if session_id in self.session_histories:
            del self.session_histories[session_id]
        
        print(f"âœ… Session {session_id[:12]}... ended and cleaned up")
    
    def clear_history(self):
        """Clear all session histories"""
        print("\nğŸ—‘ï¸ Clearing all session histories...")
        self.session_histories = {}
        self.llm_client.clear_history()
        print("âœ… All session histories cleared")